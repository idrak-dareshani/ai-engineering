# -*- coding: utf-8 -*-
"""idrak-dareshani.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eTsUnEATJ6luDGu1GbbGABzbF-apuh-p

# Lecture 6: Loading Models with `from_pretrained`
In this notebook, we will explore how to load pre-trained models using the `from_pretrained` method from the Hugging Face Transformers library. We will also dive into the configuration, weights, and caching mechanisms.

The Google Colab versin of this notebook is available here: https://colab.research.google.com/drive/1doTkD2AmY-wQwsqzUSq-gNpKgPjxsYqV

# Step 1: Load libraries and log in to Huggingface
"""

!pip install -q torch transformers bitsandbytes

import os
import torch
#from google.colab import userdata
from huggingface_hub import login

#hf_token = userdata.get('HF_TOKEN')
with open('hf_token.txt', 'r') as f:
  hf_token = f.read()

login(hf_token, add_to_git_credential=True)

"""# Step 2: Load quantization configuration for model"""

from transformers import BitsAndBytesConfig

# Quantization Config - this allows us to load the model into memory and use less memory
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4"
)

"""# Step 3: Load a Pre-trained Model
We will use the `gpt2` model as an example. This step demonstrates how to load the model and tokenizer.
"""

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", quantization_config=quant_config)
tokenizer = AutoTokenizer.from_pretrained(model_name)

print(f"Model '{model_name}' loaded successfully!")

"""# Step 4: Explore the Model Configuration
The configuration of a model contains important details such as the number of layers, hidden size, and more.
"""

config = model.config
print("Model Configuration:")
print(config)

"""Here you can see the actual layers"""

model

"""# Step 5: Understand Caching
When you load a model, it is cached locally to avoid downloading it again. The models are usually stored in the following path: ~/.cache/huggingface/hub by default

See further reference: [Huggingface cache management](~/.cache/huggingface/hub)

# Step 6: Tokenizing a Prompt and Generating Text
In this step, we will tokenize a list of messages, pass it to the model, and generate text as output.
"""

input_texts = "ایپلی کیشن جدیدیت ہے"

tokenizer.pad_token = tokenizer.eos_token

# Tokenize the input texts
inputs = tokenizer.encode(input_texts, return_tensors="pt")

with torch.no_grad():
    outputs = model.generate(
        inputs,
        max_length=inputs.shape[1] + 100,  # Generate more tokens
        temperature=0.8,
        do_sample=True,
        top_p=0.9,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id
    )

# Get only the generated part
generated_text = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
print(f"Input: {input_texts}")
print(f"Generated: {generated_text}")

"""# Takeaways
- The `from_pretrained` method simplifies loading pre-trained models and tokenizers.
- Models are cached locally for efficiency.
- You can explore model configurations and map models to devices for optimized inference.

# Your Challenge
Fork this notebook, change the model ID to one in your native language, and share your results in the course repository!
"""